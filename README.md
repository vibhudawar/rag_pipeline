# ğŸ“š RAG Pipeline

A comprehensive **Retrieval-Augmented Generation (RAG)** system that combines vector search, web search, reranking, and LLM generation to provide intelligent question-answering over your documents.

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Streamlit](https://img.shields.io/badge/streamlit-1.46.0-red.svg)

## ğŸŒŸ Features

### ğŸ” **Hybrid Retrieval**
- **Vector Search**: Semantic similarity search using Pinecone vector database
- **Web Search**: Real-time web results via SerpAPI, DuckDuckGo, or Brave
- **Smart Combination**: Automatically merges and deduplicates results

### ğŸ“„ **Document Processing**
- **Multiple Formats**: PDF, DOCX, TXT, Markdown support
- **Intelligent Chunking**: Recursive and token-based chunking strategies
- **Batch Upload**: Process multiple documents simultaneously
- **Metadata Enhancement**: Rich metadata extraction and custom tagging

### ğŸ¯ **Advanced Reranking**
- **Cohere Reranker**: State-of-the-art neural reranking
- **HuggingFace Models**: Local cross-encoder models
- **Score-based**: Fallback similarity score ranking
- **Configurable**: Choose best model for your use case

### ğŸ¤– **Multi-Provider LLM Support**
- **OpenAI**: GPT-3.5-turbo, GPT-4, GPT-4-turbo
- **Google Gemini**: Gemini Pro, Gemini Pro Vision
- **Auto-Selection**: Intelligent provider fallback

### ğŸ–¥ï¸ **User-Friendly Interface**
- **Streamlit Web App**: Beautiful, intuitive interface
- **Real-time Processing**: Live progress tracking
- **Interactive Search**: Test retrieval before querying
- **Batch Operations**: Handle multiple queries at once

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- API Keys for your chosen providers (OpenAI, Pinecone, etc.)

### 1. Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd RAG

# Install dependencies
pip install -r requirements.txt
```

### 2. Environment Setup

Create a `.env` file in the project root:

```env
# Required APIs
OPENAI_API_KEY=sk-your-openai-api-key-here
PINECONE_API_KEY=your-pinecone-api-key-here

# Optional APIs (for enhanced features)
GEMINI_API_KEY=your-gemini-api-key-here
COHERE_API_KEY=your-cohere-api-key-here
SERPAPI_KEY=your-serpapi-key-here

# Configuration (optional)
EMBEDDING_PROVIDER=openai
LLM_PROVIDER=openai
PINECONE_ENVIRONMENT=us-east-1
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
CHUNKING_STRATEGY=recursive

# RAG Pipeline Settings
INCLUDE_WEB_SEARCH=true
VECTOR_TOP_K=10
WEB_SEARCH_RESULTS=3
FINAL_TOP_K=5
RERANKER_PROVIDER=auto
```

### 3. Launch the App

```bash
streamlit run streamlit_app.py
```

Navigate to `http://localhost:8501` to access the web interface.

## ğŸ“– Usage Guide

### Document Upload

1. **Single Upload**: Upload individual documents with custom metadata
2. **Batch Upload**: Process multiple files with shared metadata
3. **Supported Formats**: PDF, DOCX, TXT, MD files
4. **Smart Processing**: Automatic format detection and optimization

### Search & Test

- **Vector Search**: Test semantic similarity search
- **Results Preview**: View retrieved chunks with scores
- **Metadata Display**: Inspect document metadata and sources

### RAG Queries

#### Single Query
```python
from src.rag_pipeline import create_rag_pipeline

# Create pipeline
pipeline = create_rag_pipeline(
    index_name="my-docs",
    include_web_search=True,
    reranker_provider="cohere"
)

# Ask questions
response = pipeline.query("What are the key benefits of machine learning?")
print(response['answer'])
```

#### Batch Processing
```python
questions = [
    "What is machine learning?",
    "How does neural networks work?",
    "What are the applications of AI?"
]

results = pipeline.batch_query(questions)
for result in results:
    print(f"Q: {result['question']}")
    print(f"A: {result['answer']}\n")
```

### Configuration Options

| Component | Options | Description |
|-----------|---------|-------------|
| **Embeddings** | `openai`, `gemini`, `auto` | Text embedding providers |
| **LLM** | `openai`, `gemini`, `auto` | Language model providers |
| **Reranker** | `cohere`, `huggingface`, `score`, `auto` | Document reranking methods |
| **Chunking** | `recursive`, `token` | Text splitting strategies |
| **Web Search** | `serpapi`, `duckduckgo`, `brave` | Web search providers |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Document      â”‚    â”‚   Vector Store   â”‚    â”‚   Web Search    â”‚
â”‚   Ingestion     â”‚â”€â”€â”€â–¶â”‚   (Pinecone)     â”‚â—€â”€â”€â–¶â”‚   (Multiple)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Chunking &    â”‚    â”‚   Embedding      â”‚    â”‚   Retrieval     â”‚
â”‚   Preprocessing â”‚    â”‚   Generation     â”‚    â”‚   Fusion        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚                        â”‚
                                 â–¼                        â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Reranking      â”‚â—€â”€â”€â”€â”‚   Query         â”‚
                        â”‚   (Cohere/HF)    â”‚    â”‚   Processing    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚                        â”‚
                                 â–¼                        â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   LLM            â”‚â—€â”€â”€â”€â”‚   Context       â”‚
                        â”‚   Generation     â”‚    â”‚   Preparation   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
RAG/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/           # Document processing
â”‚   â”‚   â”œâ”€â”€ document_parser.py
â”‚   â”‚   â”œâ”€â”€ chunking.py
â”‚   â”‚   â””â”€â”€ vector_store.py
â”‚   â”œâ”€â”€ retrieval/           # Search components
â”‚   â”‚   â”œâ”€â”€ embedder.py
â”‚   â”‚   â””â”€â”€ web_search.py
â”‚   â”œâ”€â”€ reranking/           # Result reranking
â”‚   â”‚   â””â”€â”€ reranker.py
â”‚   â”œâ”€â”€ generation/          # LLM integration
â”‚   â”‚   â””â”€â”€ llm.py
â”‚   â””â”€â”€ rag_pipeline.py      # Main pipeline
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ components.py        # Streamlit components
â”œâ”€â”€ streamlit_app.py         # Web interface
â”œâ”€â”€ config.py               # Configuration
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ README.md              # This file
```

## ğŸ§ª Testing

Run the test suite to verify your setup:

```bash
python test_rag_pipeline.py
```

This will test:
- Document ingestion
- Vector search functionality
- RAG pipeline operations
- Batch processing
- Error handling

## ğŸ”§ Advanced Configuration

### Custom Chunking

```python
from src.ingestion.chunking import get_chunker

chunker = get_chunker(
    strategy="recursive",
    chunk_size=1500,
    chunk_overlap=300
)
```

### Custom Reranking

```python
from src.reranking.reranker import CohereReranker

reranker = CohereReranker(model="rerank-english-v3.0")
```

### Pipeline Customization

```python
pipeline = create_rag_pipeline(
    index_name="custom-index",
    vector_top_k=15,
    web_search_results=5,
    final_top_k=8,
    include_web_search=True
)
```

## ğŸ“Š Monitoring & Analytics

The pipeline provides comprehensive analytics:

- **Processing Times**: Track query response times
- **Retrieval Stats**: Monitor source breakdown (vector vs web)
- **Success Rates**: Track successful vs failed queries
- **Usage Patterns**: Analyze query types and frequencies

## ğŸš¨ Troubleshooting

### Common Issues

#### "No documents retrieved"
- âœ… Verify documents are properly ingested
- âœ… Check Pinecone index exists and has data
- âœ… Try broader search terms

#### "Generation failed"
- âœ… Verify API keys are correct
- âœ… Check API rate limits
- âœ… Try different LLM provider

#### "Reranking failed"
- âœ… Check Cohere API key (if using Cohere)
- âœ… Install `sentence-transformers` for HuggingFace
- âœ… Pipeline automatically falls back to score-based ranking

#### "Web search failed"
- âœ… Verify SerpAPI key (if using SerpAPI)
- âœ… Pipeline falls back to DuckDuckGo search
- âœ… Web search failures don't stop the pipeline

### Performance Optimization

#### For Better Quality:
- Use Cohere reranker with valid API key
- Increase `final_top_k` for more context
- Enable web search for broader knowledge
- Use GPT-4 for better generation quality

#### For Better Speed:
- Reduce `vector_top_k` and `web_search_results`
- Use score-based reranker instead of neural models
- Disable web search if not needed
- Use GPT-3.5-turbo instead of GPT-4

#### For Cost Optimization:
- Use Gemini instead of OpenAI (when available)
- Reduce context size (`final_top_k`)
- Use local HuggingFace reranker instead of Cohere
- Cache frequently asked questions

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [LangChain](https://github.com/langchain-ai/langchain) for the excellent AI framework
- [Pinecone](https://www.pinecone.io/) for vector database services
- [Streamlit](https://streamlit.io/) for the beautiful web interface
- [OpenAI](https://openai.com/) and [Google](https://ai.google/) for powerful language models
- [Cohere](https://cohere.ai/) for state-of-the-art reranking

## ğŸ“ Support

If you encounter any issues or have questions:

1. Check the [Troubleshooting](#-troubleshooting) section
2. Search existing [GitHub Issues](../../issues)
3. Create a new issue with detailed description

---

**Built with â¤ï¸ for the AI community**
